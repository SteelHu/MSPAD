# DACAD é¡¹ç›®ä¸­æ–‡æ³¨é‡Šè¯´æ˜æ–‡æ¡£

> **ä½œè€…**: AI Assistant  
> **æ—¥æœŸ**: 2025-11-03  
> **ç›®çš„**: å¸®åŠ©ç†è§£ DACAD (Domain Adaptation Contrastive Learning for Anomaly Detection) é¡¹ç›®ä»£ç 

---

## ğŸ“š ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [å·²æ·»åŠ æ³¨é‡Šçš„æ–‡ä»¶](#2-å·²æ·»åŠ æ³¨é‡Šçš„æ–‡ä»¶)
3. [æ ¸å¿ƒæ¦‚å¿µè§£é‡Š](#3-æ ¸å¿ƒæ¦‚å¿µè§£é‡Š)
4. [ä»£ç é˜…è¯»é¡ºåº](#4-ä»£ç é˜…è¯»é¡ºåº)
5. [å…³é”®æŠ€æœ¯è¯¦è§£](#5-å…³é”®æŠ€æœ¯è¯¦è§£)
6. [å¸¸è§é—®é¢˜è§£ç­”](#6-å¸¸è§é—®é¢˜è§£ç­”)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 ç ”ç©¶ç›®æ ‡
- **é—®é¢˜**: æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä¸­æ ‡æ³¨æ•°æ®ç¨€ç¼º
- **è§£å†³æ–¹æ¡ˆ**: åˆ©ç”¨æœ‰æ ‡ç­¾çš„æºåŸŸæ•°æ®æ¥æ£€æµ‹æ— æ ‡ç­¾ç›®æ ‡åŸŸçš„å¼‚å¸¸
- **åˆ›æ–°ç‚¹**: ç»“åˆå¯¹æ¯”å­¦ä¹  + å¼‚å¸¸æ³¨å…¥ + åŸŸè‡ªé€‚åº”

### 1.2 åº”ç”¨åœºæ™¯
- ğŸ›°ï¸ **èˆªå¤©å™¨ç›‘æ§**: ä¸åŒå«æ˜Ÿä¼ æ„Ÿå™¨ä¹‹é—´çš„å¼‚å¸¸æ£€æµ‹è¿ç§»
- ğŸ’» **æœåŠ¡å™¨ç›‘æ§**: ä»ä¸€ä¸ªæœåŠ¡å™¨å­¦åˆ°çš„çŸ¥è¯†è¿ç§»åˆ°å…¶ä»–æœåŠ¡å™¨
- ğŸ­ **å·¥ä¸šè®¾å¤‡**: é”…ç‚‰ã€æ¶¡è½®æœºç­‰è®¾å¤‡çš„æ•…éšœæ£€æµ‹

---

## 2. å·²æ·»åŠ æ³¨é‡Šçš„æ–‡ä»¶

### âœ… å·²å®Œæˆè¯¦ç»†æ³¨é‡Š

| æ–‡ä»¶è·¯å¾„ | åŠŸèƒ½ | æ³¨é‡Šå†…å®¹ |
|---------|------|----------|
| `main/main_MSL.py` | å®éªŒå…¥å£ | âœ… å®Œæ•´ä¸­æ–‡æ³¨é‡Š + å‚æ•°è¯´æ˜ |
| `main/train.py` | è®­ç»ƒæµç¨‹ | âœ… 8ä¸ªè®­ç»ƒæ­¥éª¤è¯¦ç»†æ³¨é‡Š |
| `main/algorithms.py` | ç®—æ³•å°è£… | âœ… DACAD æ ¸å¿ƒé€»è¾‘æ³¨é‡Š |
| `main/models/dacad.py` | æ¨¡å‹æ¶æ„ | âœ… MoCo æœºåˆ¶è¯¦ç»†æ³¨é‡Š |

### ğŸ“ é‡è¦ä½†æœªå®Œå…¨æ³¨é‡Šçš„æ–‡ä»¶

- `utils/dataset.py`: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼ˆéƒ¨åˆ†æ³¨é‡Šï¼‰
- `utils/loss.py`: æŸå¤±å‡½æ•°å®šä¹‰ï¼ˆå¾…æ³¨é‡Šï¼‰
- `utils/augmentations.py`: å¼‚å¸¸æ³¨å…¥æœºåˆ¶ï¼ˆå¾…æ³¨é‡Šï¼‰

---

## 3. æ ¸å¿ƒæ¦‚å¿µè§£é‡Š

### 3.1 è·¨åŸŸå¼‚å¸¸æ£€æµ‹ (Cross-Domain Anomaly Detection)

```
æºåŸŸ (Source Domain)               ç›®æ ‡åŸŸ (Target Domain)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  F-5 é€šé“æ•°æ®       â”‚            â”‚  C-1 é€šé“æ•°æ®       â”‚
â”‚  âœ… æœ‰æ ‡ç­¾          â”‚  ======>   â”‚  âŒ æ— æ ‡ç­¾          â”‚
â”‚  æ­£å¸¸: 5000 ä¸ª     â”‚   è¿ç§»      â”‚  éœ€è¦æ£€æµ‹å¼‚å¸¸       â”‚
â”‚  å¼‚å¸¸: 500 ä¸ª      â”‚            â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®æŒ‘æˆ˜**: æºåŸŸå’Œç›®æ ‡åŸŸçš„æ•°æ®åˆ†å¸ƒä¸åŒï¼ˆåŸŸåç§»ï¼‰

**DACAD è§£å†³æ–¹æ¡ˆ**:
1. **åŸŸå¯¹æŠ—è®­ç»ƒ**: å­¦ä¹ åŸŸä¸å˜ç‰¹å¾
2. **å¯¹æ¯”å­¦ä¹ **: æ‹‰è¿‘æ­£å¸¸æ ·æœ¬ï¼Œæ¨è¿œå¼‚å¸¸æ ·æœ¬
3. **å¼‚å¸¸æ³¨å…¥**: åœ¨ç›®æ ‡åŸŸç”Ÿæˆä¼ªå¼‚å¸¸è¿›è¡Œè®­ç»ƒ

---

### 3.2 DACAD çš„äº”ä¸ªæŸå¤±å‡½æ•°

#### 1ï¸âƒ£ åŸŸåˆ¤åˆ«å™¨æŸå¤± (Domain Adversarial Loss)
```python
loss_disc = BCE(åˆ¤åˆ«å™¨è¾“å‡º, åŸŸæ ‡ç­¾)
æƒé‡: 0.5
```
- **ç›®çš„**: è®©ç‰¹å¾æå–å™¨å­¦ä¹ åŸŸä¸å˜çš„ç‰¹å¾
- **æœºåˆ¶**: åˆ¤åˆ«å™¨è¯•å›¾åŒºåˆ†æºåŸŸ/ç›®æ ‡åŸŸï¼Œç¼–ç å™¨è¯•å›¾æ··æ·†åˆ¤åˆ«å™¨

#### 2ï¸âƒ£ Deep SVDD åˆ†ç±»æŸå¤±
```python
src_cls_loss = ||ç‰¹å¾ - ä¸­å¿ƒ||Â² (æ­£å¸¸æ ·æœ¬åº”é è¿‘ä¸­å¿ƒ)
æƒé‡: 1.0
```
- **ç›®çš„**: åœ¨ç‰¹å¾ç©ºé—´å­¦ä¹ ä¸€ä¸ªè¶…çƒé¢è¾¹ç•Œ
- **åŸç†**: æ­£å¸¸æ ·æœ¬åœ¨çƒå†…ï¼Œå¼‚å¸¸æ ·æœ¬åœ¨çƒå¤–

#### 3ï¸âƒ£ æºåŸŸç›‘ç£å¯¹æ¯”æŸå¤± (Supervised Contrastive Loss)
```python
src_sup_cont_loss = Triplet(é”šç‚¹, æ­£æ ·æœ¬, è´Ÿæ ·æœ¬)
æƒé‡: 0.1
```
- **ä¸‰å…ƒç»„**:
  - é”šç‚¹: å½“å‰æ ·æœ¬
  - æ­£æ ·æœ¬: åŒç±»æ ·æœ¬ï¼ˆéƒ½æ­£å¸¸æˆ–éƒ½å¼‚å¸¸ï¼‰
  - è´Ÿæ ·æœ¬: å¼‚å¸¸æ ·æœ¬

#### 4ï¸âƒ£ ç›®æ ‡åŸŸå¼‚å¸¸æ³¨å…¥å¯¹æ¯”æŸå¤±
```python
trg_inj_cont_loss = Triplet(é”šç‚¹, æ­£æ ·æœ¬, æ³¨å…¥çš„å¼‚å¸¸)
æƒé‡: 0.1
```
- **å…³é”®**: ä½¿ç”¨äººå·¥æ³¨å…¥çš„å¼‚å¸¸ä½œä¸ºè´Ÿæ ·æœ¬
- **ä¼˜åŠ¿**: ä¸éœ€è¦çœŸå®çš„å¼‚å¸¸æ ‡ç­¾

#### 5ï¸âƒ£ æ€»æŸå¤±
```python
total_loss = 0.5*loss_disc + 1.0*src_cls_loss 
           + 0.1*src_sup_cont + 0.1*trg_inj_cont
```

---

### 3.3 MoCo (Momentum Contrast) æœºåˆ¶

DACAD ä½¿ç”¨ MoCo æ¡†æ¶æ¥å®ç°é«˜æ•ˆçš„å¯¹æ¯”å­¦ä¹ ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MoCo æœºåˆ¶                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                              â”‚
â”‚  1. åŒç¼–ç å™¨:                                â”‚
â”‚     encoder_q (æŸ¥è¯¢ç¼–ç å™¨) â† æ¢¯åº¦æ›´æ–°        â”‚
â”‚     encoder_k (é”®ç¼–ç å™¨)   â† åŠ¨é‡æ›´æ–°        â”‚
â”‚                                              â”‚
â”‚  2. åŠ¨é‡æ›´æ–°å…¬å¼:                            â”‚
â”‚     Î¸_k = 0.999 * Î¸_k + 0.001 * Î¸_q        â”‚
â”‚                                              â”‚
â”‚  3. ç‰¹å¾é˜Ÿåˆ—:                                â”‚
â”‚     å­˜å‚¨ 98,304 ä¸ªè´Ÿæ ·æœ¬ç‰¹å¾                 â”‚
â”‚     é‡‡ç”¨ FIFO ç­–ç•¥æ›´æ–°                       â”‚
â”‚                                              â”‚
â”‚  4. ä¼˜åŠ¿:                                    â”‚
â”‚     âœ… æä¾›å¤§é‡è´Ÿæ ·æœ¬                        â”‚
â”‚     âœ… é”®ç‰¹å¾æ›´ç¨³å®š                          â”‚
â”‚     âœ… å¯¹æ¯”å­¦ä¹ æ•ˆæœæ›´å¥½                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ä»£ç ä½ç½®**: 
- `models/dacad.py:242-264` (åŠ¨é‡æ›´æ–°)
- `models/dacad.py:267-295` (é˜Ÿåˆ—æ›´æ–°)

---

### 3.4 å¼‚å¸¸æ³¨å…¥æœºåˆ¶ (Anomaly Injection)

åœ¨ç›®æ ‡åŸŸæ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡äººå·¥æ³¨å…¥å¼‚å¸¸æ¥è®­ç»ƒæ¨¡å‹ï¼š

```python
def get_injector(sequence, mean, std):
    """
    å¼‚å¸¸æ³¨å…¥æ–¹æ³•ï¼š
    1. æ·»åŠ é«˜æ–¯å™ªå£°
    2. æ”¹å˜å¹…åº¦
    3. æ’å…¥å¼‚å¸¸æ¨¡å¼
    """
    # ç¤ºä¾‹ï¼šæ·»åŠ å™ªå£°
    injected = sequence + np.random.randn(*sequence.shape) * std * 2
    return injected
```

**ä½¿ç”¨ä½ç½®**: 
- `utils/dataset.py:60` - æ•°æ®åŠ è½½æ—¶ 90% æ¦‚ç‡ä½¿ç”¨æ³¨å…¥å¼‚å¸¸
- `algorithms.py:272-277` - è®¡ç®—ç›®æ ‡åŸŸå¯¹æ¯”æŸå¤±æ—¶ä½¿ç”¨

---

## 4. ä»£ç é˜…è¯»é¡ºåº

### ğŸ¯ æ¨èå­¦ä¹ è·¯å¾„ (3-5 å¤©)

#### **ç¬¬1å¤©: æ•´ä½“ç†è§£**
1. âœ… `README.md` - äº†è§£é¡¹ç›®èƒŒæ™¯
2. âœ… `main/main_MSL.py` (å·²æ³¨é‡Š) - å®éªŒå…¥å£
3. âœ… `utils/dataset.py:12-82` - æ•°æ®åŠ è½½

#### **ç¬¬2å¤©: è®­ç»ƒæµç¨‹**
4. âœ… `main/train.py` (å·²æ³¨é‡Š) - è®­ç»ƒä¸»å¾ªç¯
5. âœ… `main/eval.py` - è¯„ä¼°æµç¨‹

#### **ç¬¬3å¤©: ç®—æ³•æ ¸å¿ƒ**
6. âœ… `main/algorithms.py:140-298` (å·²æ³¨é‡Š) - DACAD ç®—æ³•
7. âœ… `utils/loss.py` - æŸå¤±å‡½æ•°

#### **ç¬¬4å¤©: æ¨¡å‹æ¶æ„**
8. âœ… `main/models/dacad.py` (å·²æ³¨é‡Š) - æ¨¡å‹å®šä¹‰
9. âœ… `utils/tcn_no_norm.py` - TCN ç½‘ç»œ
10. âœ… `utils/mlp.py` - MLP ç»„ä»¶

#### **ç¬¬5å¤©: æ•°æ®å¢å¼º**
11. âœ… `utils/augmentations.py` - å¼‚å¸¸æ³¨å…¥

---

## 5. å…³é”®æŠ€æœ¯è¯¦è§£

### 5.1 æ—¶é—´å·ç§¯ç½‘ç»œ (TCN)

```
TCN ç»“æ„ï¼ˆä»¥ num_channels=[128, 256, 512] ä¸ºä¾‹ï¼‰ï¼š

è¾“å…¥: [batch, channels, seq_len]
  â”‚
  â”œâ”€> TCN Block 1 (128 channels, dilation=1)
  â”‚     â””â”€> Conv1d â†’ ReLU â†’ Dropout
  â”‚
  â”œâ”€> TCN Block 2 (256 channels, dilation=3)
  â”‚     â””â”€> Conv1d â†’ ReLU â†’ Dropout
  â”‚
  â””â”€> TCN Block 3 (512 channels, dilation=9)
        â””â”€> Conv1d â†’ ReLU â†’ Dropout
        
è¾“å‡º: [batch, 512, seq_len]
å–æœ€åæ—¶é—´æ­¥: [batch, 512]
```

**ä¼˜åŠ¿**:
- âœ… å¤„ç†é•¿åºåˆ—æ•°æ®
- âœ… å¹¶è¡Œè®¡ç®—æ•ˆç‡é«˜
- âœ… è†¨èƒ€å·ç§¯æ‰©å¤§æ„Ÿå—é‡

---

### 5.2 Deep SVDD åŸç†

```
ç‰¹å¾ç©ºé—´ä¸­çš„è¶…çƒé¢è¾¹ç•Œï¼š

        å¼‚å¸¸ â—
              
    æ­£å¸¸ â—‹  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    æ­£å¸¸ â—‹  â”‚         â”‚ â† è¶…çƒé¢
    æ­£å¸¸ â—‹  â”‚  âŠ—ä¸­å¿ƒ  â”‚
    æ­£å¸¸ â—‹  â”‚         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  
        å¼‚å¸¸ â—

æŸå¤±å‡½æ•°:
- æ­£å¸¸æ ·æœ¬: æœ€å°åŒ– ||ç‰¹å¾ - ä¸­å¿ƒ||Â²
- å¼‚å¸¸æ ·æœ¬: æœ€å¤§åŒ– ||ç‰¹å¾ - ä¸­å¿ƒ||Â²
```

**ä»£ç ä½ç½®**: `algorithms.py:262`

---

### 5.3 æ¢¯åº¦åè½¬å±‚ (Gradient Reversal)

```python
class ReverseLayerF:
    def forward(x):
        return x  # å‰å‘ä¼ æ’­ï¼šä¸æ”¹å˜
    
    def backward(grad):
        return -Î± * grad  # åå‘ä¼ æ’­ï¼šæ¢¯åº¦å–å
```

**ä½œç”¨**: 
- åˆ¤åˆ«å™¨: åŠªåŠ›åŒºåˆ†æºåŸŸ/ç›®æ ‡åŸŸ (æ¢¯åº¦æ­£å¸¸)
- ç‰¹å¾æå–å™¨: åŠªåŠ›æ··æ·†åˆ¤åˆ«å™¨ (æ¢¯åº¦åè½¬)
- ç»“æœ: å­¦åˆ°åŸŸä¸å˜çš„ç‰¹å¾

---

## 6. å¸¸è§é—®é¢˜è§£ç­”

### Q1: ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªç¼–ç å™¨ï¼Ÿ

**A**: 
- `encoder_q`: é€šè¿‡æ¢¯åº¦æ›´æ–°ï¼Œå¿«é€Ÿé€‚åº”å½“å‰æ•°æ®
- `encoder_k`: é€šè¿‡åŠ¨é‡æ›´æ–°ï¼Œæä¾›ç¨³å®šçš„ç‰¹å¾
- å¥½å¤„: é”®ç‰¹å¾ä¸ä¼šå˜åŒ–å¤ªå¿«ï¼Œæå‡å¯¹æ¯”å­¦ä¹ çš„ä¸€è‡´æ€§

---

### Q2: é˜Ÿåˆ—çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

**A**: 
- å­˜å‚¨ 98,304 ä¸ªè´Ÿæ ·æœ¬ç‰¹å¾
- æ¯”ä»…ä½¿ç”¨å½“å‰ batchï¼ˆ256ä¸ªï¼‰çš„è´Ÿæ ·æœ¬å¤š 384 å€
- æä¾›æ›´å¤šå¯¹æ¯”ä¿¡å·ï¼Œæ˜¾è‘—æå‡æ€§èƒ½

---

### Q3: å¦‚ä½•ç†è§£"å¼‚å¸¸æ³¨å…¥"ï¼Ÿ

**A**: ç›®æ ‡åŸŸæ²¡æœ‰å¼‚å¸¸æ ‡ç­¾æ—¶ï¼š
```python
# å‡è®¾ç›®æ ‡åŸŸæ ·æœ¬éƒ½æ˜¯æ­£å¸¸çš„
anchor = æ­£å¸¸æ ·æœ¬
positive = åŒä¸€ä¸ªæ­£å¸¸æ ·æœ¬ï¼ˆæˆ–å…¶å¢å¼ºç‰ˆæœ¬ï¼‰
negative = æ³¨å…¥å¼‚å¸¸çš„ç‰ˆæœ¬ï¼ˆæ·»åŠ å™ªå£°ç­‰ï¼‰

# è®­ç»ƒç›®æ ‡ï¼šè®©æ¨¡å‹å­¦ä¼šåŒºåˆ†æ­£å¸¸å’Œæ³¨å…¥çš„å¼‚å¸¸
loss = Triplet(anchor, positive, negative)
```

---

### Q4: ä¸ºä»€ä¹ˆæºåŸŸå’Œç›®æ ‡åŸŸçš„è®­ç»ƒç­–ç•¥ä¸åŒï¼Ÿ

**A**:
| è®­ç»ƒç­–ç•¥ | æºåŸŸ | ç›®æ ‡åŸŸ |
|---------|------|--------|
| **æ ‡ç­¾** | âœ… æœ‰æ ‡ç­¾ | âŒ æ— æ ‡ç­¾ |
| **å¯¹æ¯”å­¦ä¹ ** | ç›‘ç£ï¼ˆç”¨çœŸå®æ ‡ç­¾ï¼‰ | è‡ªç›‘ç£ï¼ˆç”¨æ³¨å…¥å¼‚å¸¸ï¼‰ |
| **æŸå¤±å‡½æ•°** | Deep SVDD + ç›‘ç£å¯¹æ¯” | æ³¨å…¥å¯¹æ¯” |
| **åŸŸå¯¹æŠ—** | âœ… å‚ä¸ | âœ… å‚ä¸ |

---

### Q5: è®­ç»ƒå¥½çš„æ¨¡å‹å¦‚ä½•ä½¿ç”¨ï¼Ÿ

```python
# 1. åŠ è½½æ¨¡å‹
algorithm = DACAD(args, input_dim, static_dim)
algorithm.load_state("results/MSL/F-5-C-1/")

# 2. åœ¨ç›®æ ‡åŸŸæµ‹è¯•é›†ä¸Šé¢„æµ‹
algorithm.eval()
for batch in test_loader:
    predictions = algorithm.predict_trg(batch)
    # predictions: å¼‚å¸¸åˆ†æ•°ï¼ˆè¶Šå¤§è¶Šå¯èƒ½æ˜¯å¼‚å¸¸ï¼‰

# 3. è¯„ä¼°æ€§èƒ½
metrics = algorithm.pred_meter_val_trg.get_metrics()
print(f"AUPRC: {metrics['avg_prc']:.4f}")
print(f"F1: {metrics['best_f1']:.4f}")
```

---

### Q6: å¦‚ä½•ä¿®æ”¹ä»£ç é€‚é…è‡ªå·±çš„æ•°æ®ï¼Ÿ

**æ­¥éª¤**:

1. **å‡†å¤‡æ•°æ®æ ¼å¼**:
```python
# æ•°æ®æ–‡ä»¶ç»“æ„
datasets/
â””â”€â”€ MyDataset/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ entity1.npy  # shape: [æ—¶é—´æ­¥, ç‰¹å¾ç»´åº¦]
    â”‚   â””â”€â”€ entity2.npy
    â”œâ”€â”€ test/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ labels.csv       # åŒ…å«å¼‚å¸¸æ ‡æ³¨
```

2. **åˆ›å»º Dataset ç±»**:
```python
# åœ¨ utils/dataset.py ä¸­æ·»åŠ 
class MyDataset(Dataset):
    def __init__(self, root_dir, subject_id, split_type):
        # åŠ è½½æ•°æ®
        self.sequence = np.load(...)
        self.label = ...
    
    def __getitem__(self, idx):
        return {
            'sequence': self.sequence[idx],
            'label': self.label[idx],
            'positive': ...,  # æ­£æ ·æœ¬
            'negative': ...   # è´Ÿæ ·æœ¬
        }
```

3. **ä¿®æ”¹ get_dataset å‡½æ•°**:
```python
def get_dataset(args, domain_type, split_type):
    if "MyDataset" in args.path_src:
        if domain_type == "source":
            return MyDataset(args.path_src, args.id_src, split_type)
        else:
            return MyDataset(args.path_trg, args.id_trg, split_type)
```

4. **åˆ›å»ºæ–°çš„å…¥å£æ–‡ä»¶**:
```python
# main/main_MyDataset.py
for src in ['entity1']:
    for trg in ['entity2']:
        command = ['python', 'main/train.py',
                   '--path_src', 'datasets/MyDataset',
                   '--path_trg', 'datasets/MyDataset',
                   '--id_src', src,
                   '--id_trg', trg,
                   ...]
        subprocess.run(command)
```

---

## 7. å®éªŒç»“æœè¯´æ˜

### 7.1 è®­ç»ƒæ—¥å¿—è§£è¯»

```bash
# è®­ç»ƒè¾“å‡ºç¤ºä¾‹
Epoch: [0]
L_Src_Sup: 2.3456    # æºåŸŸç›‘ç£å¯¹æ¯”æŸå¤±
L_Trg_Inj: 1.8923    # ç›®æ ‡åŸŸæ³¨å…¥å¯¹æ¯”æŸå¤±
Loss Disc: 0.4567    # åŸŸåˆ¤åˆ«å™¨æŸå¤±
Loss Pred: 3.2145    # Deep SVDD æŸå¤±
Loss TOTAL: 5.1234   # æ€»æŸå¤±

VALIDATION RESULTS
VALIDATION SOURCE PREDICTIONS
AUPRC score is : 0.7184   # ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ä¸‹é¢ç§¯
Best F1 score is : 0.7983  # æœ€ä½³ F1 åˆ†æ•°

VALIDATION TARGET PREDICTIONS
AUPRC score is : 0.5479
Best F1 score is : 0.5114

*** New best model saved! ***  # ä¿å­˜æœ€ä½³æ¨¡å‹
```

### 7.2 æ€§èƒ½æŒ‡æ ‡è¯´æ˜

| æŒ‡æ ‡ | è¯´æ˜ | å–å€¼èŒƒå›´ | ç†æƒ³å€¼ |
|-----|------|----------|--------|
| **AUPRC** | ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ä¸‹é¢ç§¯ | [0, 1] | è¶Šé«˜è¶Šå¥½ |
| **F1** | ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ | [0, 1] | è¶Šé«˜è¶Šå¥½ |
| **Precision** | é¢„æµ‹ä¸ºå¼‚å¸¸ä¸­çœŸæ­£å¼‚å¸¸çš„æ¯”ä¾‹ | [0, 1] | è¶Šé«˜è¶Šå¥½ |
| **Recall** | çœŸå®å¼‚å¸¸ä¸­è¢«æ£€æµ‹å‡ºçš„æ¯”ä¾‹ | [0, 1] | è¶Šé«˜è¶Šå¥½ |

---

## 8. è°ƒè¯•æŠ€å·§

### 8.1 æŸ¥çœ‹æ•°æ®å½¢çŠ¶

åœ¨å…³é”®ä½ç½®æ·»åŠ æ‰“å°ï¼š

```python
# åœ¨ train.py:178 åæ·»åŠ 
print(f"æºåŸŸæ•°æ®å½¢çŠ¶: {sample_batched_src['sequence'].shape}")
print(f"ç›®æ ‡åŸŸæ•°æ®å½¢çŠ¶: {sample_batched_trg['sequence'].shape}")
# è¾“å‡º: [batch_size=256, seq_len=100, features=55]
```

### 8.2 å¯è§†åŒ–ç‰¹å¾

```python
# åœ¨ algorithms.py ä¸­æ·»åŠ 
def visualize_features(features, labels, save_path):
    from sklearn.manifold import TSNE
    import matplotlib.pyplot as plt
    
    # é™ç»´åˆ° 2D
    tsne = TSNE(n_components=2)
    features_2d = tsne.fit_transform(features.cpu().numpy())
    
    # ç»˜å›¾
    plt.scatter(features_2d[labels==0, 0], features_2d[labels==0, 1], 
                c='blue', label='Normal')
    plt.scatter(features_2d[labels==1, 0], features_2d[labels==1, 1], 
                c='red', label='Anomaly')
    plt.legend()
    plt.savefig(save_path)
```

### 8.3 ç›‘æ§æŸå¤±å˜åŒ–

```python
# åˆ›å»º TensorBoard æ—¥å¿—
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter('runs/experiment1')

# åœ¨è®­ç»ƒå¾ªç¯ä¸­è®°å½•
writer.add_scalar('Loss/disc', loss_disc.item(), count_step)
writer.add_scalar('Loss/pred', src_cls_loss.item(), count_step)
writer.add_scalar('Metrics/F1', metrics['best_f1'], count_step)
```

---

## 9. å‚è€ƒèµ„æº

### 9.1 è®ºæ–‡

```bibtex
@article{darban2025dacad,
  title={DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series},
  author={Darban, Zahra Zamanzadeh and Yang, Yiyuan and Webb, Geoffrey I. and Aggarwal, Charu C. and Wen, Qingsong and Salehi, Mahsa},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2025}
}
```

### 9.2 ç›¸å…³æŠ€æœ¯

- **MoCo**: "Momentum Contrast for Unsupervised Visual Representation Learning" (CVPR 2020)
- **Deep SVDD**: "Deep One-Class Classification" (ICML 2018)
- **Domain Adaptation**: "Domain-Adversarial Training of Neural Networks" (JMLR 2016)

---

## 10. è”ç³»æ–¹å¼

å¦‚æœæ‚¨åœ¨ç†è§£ä»£ç æ—¶é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ï¼š

1. æŸ¥çœ‹é¡¹ç›® README.md
2. é˜…è¯»åŸå§‹è®ºæ–‡
3. æŸ¥çœ‹ä»£ç ä¸­çš„è¯¦ç»†æ³¨é‡Š
4. GitHub Issues æé—®

---

**ç¥æ‚¨å­¦ä¹ é¡ºåˆ©ï¼ğŸ‰**

---

*æœ€åæ›´æ–°: 2025-11-03*

