# 评估指标说明文档

本文档详细描述了DACAD项目中使用的所有评估指标，包括其含义、计算公式和使用场景。

## 目录

1. [概述](#概述)
2. [二分类异常检测指标](#二分类异常检测指标)
3. [多分类指标](#多分类指标)
4. [数据集类型与指标选择](#数据集类型与指标选择)
5. [指标计算实现](#指标计算实现)
6. [参考](#参考)

---

## 概述

本项目主要针对时间序列异常检测任务，支持多种数据集类型（SMD、MSL、Boiler、Sensor等）。根据不同的数据集类型和任务特点，项目采用了不同的评估指标组合。

### 主要评估指标列表

| 指标名称 | 英文名称 | 代码标识 | 适用场景 |
|---------|---------|---------|---------|
| 平均精度 | Average Precision | `avg_prc` | 异常检测（SMD/MSL/Boiler） |
| ROC曲线下面积 | ROC AUC | `roc_auc` | 异常检测 |
| 最佳F1分数 | Best F1 Score | `best_f1` | 异常检测 |
| 最佳精确率 | Best Precision | `best_prec` | 异常检测 |
| 最佳召回率 | Best Recall | `best_rec` | 异常检测 |
| 最佳阈值 | Best Threshold | `best_thr` | 异常检测 |
| 宏平均F1 | Macro F1 | `macro_F1` | 多分类任务 |
| 准确率 | Accuracy | `acc` | 通用分类任务 |
| 加权F1 | Weighted F1 | `w_f1` | 多分类任务（类别不平衡） |

---

## 二分类异常检测指标

### 1. AUPRC (Average Precision) - 平均精度

**指标标识**: `avg_prc`

**含义**: 
AUPRC（Average Precision）是精确率-召回率曲线（PR曲线）下的面积，用于衡量模型在正类（异常类）上的平均精度。在类别不平衡的情况下，AUPRC比ROC AUC更能反映模型的真实性能。

**计算公式**:
```
AUPRC = ∫₀¹ P(R) dR
```
其中：
- P(R) 表示在召回率为R时的精确率
- 积分计算PR曲线下的面积

**实现方式**:
使用sklearn的`average_precision_score`函数：
```python
avg_prc = average_precision_score(target, output, pos_label=1)
```

**使用场景**:
- SMD数据集的主要评估指标
- MSL数据集的主要评估指标
- Boiler数据集的主要评估指标

**取值范围**: [0, 1]，值越大越好

---

### 2. ROC AUC - ROC曲线下面积

**指标标识**: `roc_auc`

**含义**:
ROC AUC（Receiver Operating Characteristic Area Under Curve）是ROC曲线下的面积，用于衡量分类器区分正负样本的能力。ROC曲线以假正率（FPR）为横轴，真正率（TPR）为纵轴。

**计算公式**:
```
ROC AUC = ∫₀¹ TPR(FPR) dFPR
```
其中：
- TPR (True Positive Rate) = TP / (TP + FN) = Recall
- FPR (False Positive Rate) = FP / (FP + TN)

**实现方式**:
使用sklearn的`roc_auc_score`函数：
```python
roc_auc = roc_auc_score(target, output)
```

**使用场景**:
- 异常检测任务的辅助评估指标
- 用于对比不同模型的性能

**取值范围**: [0, 1]，值越大越好

---

### 3. Best F1 Score - 最佳F1分数

**指标标识**: `best_f1`

**含义**:
在所有可能的分类阈值下，选择使F1分数达到最大值时的F1分数。F1分数是精确率和召回率的调和平均数，能够平衡精确率和召回率。

**计算公式**:
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```
其中：
- Precision = TP / (TP + FP)
- Recall = TP / (TP + FN)

最佳F1分数通过遍历所有阈值得到：
```
best_f1 = max_threshold F1(threshold)
```

**实现方式**:
```python
prec, rec, thr = precision_recall_curve(target, output, pos_label=1)
f1score = np.where((rec + prec) == 0.0, 0.0, 2 * prec * rec / (prec + rec))
best_f1_index = np.argmax(f1score)
best_f1 = f1score[best_f1_index]
```

**使用场景**:
- SMD、MSL、Boiler数据集的重要评估指标
- 需要平衡精确率和召回率的场景

**取值范围**: [0, 1]，值越大越好

---

### 4. Best Precision - 最佳精确率

**指标标识**: `best_prec`

**含义**:
在最佳F1分数对应的阈值下，模型的精确率（Precision）。精确率表示在所有被预测为正类的样本中，真正为正类的比例。

**计算公式**:
```
Precision = TP / (TP + FP)
```
其中：
- TP (True Positive): 真正例
- FP (False Positive): 假正例

**实现方式**:
```python
best_prec = prec[best_f1_index]
```

**使用场景**:
- 评估模型在最佳阈值下的精确率
- 与最佳召回率一起使用，全面评估模型性能

**取值范围**: [0, 1]，值越大越好

---

### 5. Best Recall - 最佳召回率

**指标标识**: `best_rec`

**含义**:
在最佳F1分数对应的阈值下，模型的召回率（Recall）。召回率表示在所有真正的正类样本中，被正确预测为正类的比例。

**计算公式**:
```
Recall = TP / (TP + FN)
```
其中：
- TP (True Positive): 真正例
- FN (False Negative): 假负例

**实现方式**:
```python
best_rec = rec[best_f1_index]
```

**使用场景**:
- 评估模型在最佳阈值下的召回率
- 与最佳精确率一起使用，全面评估模型性能

**取值范围**: [0, 1]，值越大越好

---

### 6. Best Threshold - 最佳阈值

**指标标识**: `best_thr`

**含义**:
使F1分数达到最大值时的分类阈值。该阈值用于将模型的输出概率转换为二分类结果。

**计算公式**:
```
best_thr = argmax_threshold F1(threshold)
```

**实现方式**:
```python
best_thr = thr[best_f1_index]
```

**使用场景**:
- 确定最优的分类阈值
- 用于将模型输出转换为最终的分类结果

**取值范围**: 通常为 [0, 1]（取决于模型输出的概率范围）

---

## 多分类指标

### 7. Macro F1 - 宏平均F1分数

**指标标识**: `macro_F1`

**含义**:
宏平均F1分数是对所有类别的F1分数进行未加权平均。每个类别的F1分数具有相同的权重，不考虑类别的样本数量。

**计算公式**:
```
Macro F1 = (1/C) × Σᵢ F1ᵢ
```
其中：
- C: 类别总数
- F1ᵢ: 第i个类别的F1分数

**实现方式**:
使用sklearn的`f1_score`函数，设置`average="macro"`：
```python
macro_F1 = f1_score(target, output, average="macro")
```

**使用场景**:
- 多分类任务的主要评估指标
- 需要平等对待所有类别的场景
- Sensor类型数据集的主要评估指标

**取值范围**: [0, 1]，值越大越好

---

### 8. Accuracy - 准确率

**指标标识**: `acc`

**含义**:
准确率表示正确预测的样本数占总样本数的比例，是最直观的分类性能指标。

**计算公式**:
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```
其中：
- TP: 真正例
- TN: 真负例
- FP: 假正例
- FN: 假负例

**实现方式**:
使用sklearn的`accuracy_score`函数：
```python
acc = accuracy_score(target, output)
```

**使用场景**:
- 多分类任务的辅助评估指标
- Sensor类型数据集的评估指标之一
- 类别平衡时的有效指标

**取值范围**: [0, 1]，值越大越好

**注意**: 在类别不平衡的情况下，准确率可能不是最佳指标。

---

### 9. Weighted F1 - 加权F1分数

**指标标识**: `w_f1`

**含义**:
加权F1分数是对所有类别的F1分数进行加权平均，权重为每个类别的样本数量。这样可以更好地反映类别不平衡情况下的模型性能。

**计算公式**:
```
Weighted F1 = Σᵢ (nᵢ/n) × F1ᵢ
```
其中：
- nᵢ: 第i个类别的样本数量
- n: 总样本数量
- F1ᵢ: 第i个类别的F1分数

**实现方式**:
使用sklearn的`f1_score`函数，设置`average="weighted"`：
```python
w_f1 = f1_score(target, output, average="weighted")
```

**使用场景**:
- 多分类任务中类别不平衡的场景
- Sensor类型数据集的评估指标之一

**取值范围**: [0, 1]，值越大越好

---

## 数据集类型与指标选择

根据不同的数据集类型，项目会自动选择相应的主要评估指标：

### SMD数据集
- **主要指标**: `avg_prc` (AUPRC)
- **辅助指标**: `best_f1`, `best_prec`, `best_rec`, `roc_auc`

### MSL数据集
- **主要指标**: `avg_prc` (AUPRC)
- **辅助指标**: `best_f1`, `best_prec`, `best_rec`, `roc_auc`

### Boiler数据集
- **主要指标**: `avg_prc` (AUPRC)
- **辅助指标**: `best_f1`, `best_prec`, `best_rec`, `roc_auc`

### Sensor数据集（其他）
- **主要指标**: `macro_F1` (宏平均F1)
- **辅助指标**: `acc`, `w_f1`

### 代码实现位置

指标选择逻辑位于 `main/algorithms.py` 和 `main_new/algorithms.py` 的 `Base_Algorithm` 类中：

```python
# 根据数据集类型选择主要评估指标
if self.dataset_type == "smd":
    self.main_pred_metric = "avg_prc"      # SMD: 使用 AUPRC
elif self.dataset_type == "msl":
    self.main_pred_metric = "avg_prc"      # MSL: 使用 AUPRC
elif self.dataset_type == "boiler":
    self.main_pred_metric = "avg_prc"      # Boiler: 使用 AUPRC
else:
    self.main_pred_metric = "mac_f1"       # 其他: 使用宏平均 F1
```

---

## 指标计算实现

### 核心实现类

所有评估指标的计算都在 `PredictionMeter` 类中实现，该类位于 `utils/util_progress_log.py` 文件中。

### 主要方法

#### `update()` 方法
用于更新预测结果和真实标签：
```python
def update(self, target, output, id_patient=None, stay_hour=None):
    # 根据数据集类型处理输出
    # 保存预测结果和真实标签
```

#### `get_metrics()` 方法
计算并返回所有评估指标：
```python
def get_metrics(self):
    return_dict = {}
    # 计算 AUPRC
    avg_prc = average_precision_score(target, output, pos_label=1)
    
    # 计算 ROC AUC
    roc_auc = roc_auc_score(target, output)
    
    # 计算精确率-召回率曲线
    prec, rec, thr = precision_recall_curve(target, output, pos_label=1)
    
    # 计算最佳F1及相关指标
    f1score = 2 * prec * rec / (prec + rec)
    best_f1_index = np.argmax(f1score)
    
    # 返回所有指标
    return return_dict
```

### 完整代码位置

```122:148:utils/util_progress_log.py
def get_metrics(self):
    return_dict = {}
    output = np.array(self.output_list)
    target = np.array(self.target_list)

    avg_prc = average_precision_score(target, output, pos_label=1)
    roc_auc =0
    try:
        roc_auc = roc_auc_score(target, output)
    except ValueError:
        pass
    prec, rec, thr = precision_recall_curve(target, output, pos_label=1)
    prec = np.where(prec == np.nan, 0.0, prec)
    rec = np.where(rec == np.nan, 0.0, rec)

    with np.errstate(invalid='ignore'):
        f1score = np.where((rec + prec) == 0.0, 0.0, 2 * prec * rec / (prec + rec))
    best_f1_index = np.argmax(f1score)
    return_dict["best_f1"] = f1score[best_f1_index]
    return_dict["best_prec"] = prec[best_f1_index]
    return_dict["best_rec"] = rec[best_f1_index]
    return_dict["best_thr"] = thr[best_f1_index]
    return_dict["avg_prc"] = avg_prc
    return_dict["roc_auc"] = roc_auc
    output = np.where(output[:] > thr[best_f1_index], 1, 0)
    return_dict["macro_F1"] = f1_score(target, output, average="macro")
    return return_dict
```

### 评估脚本

评估指标在以下脚本中被使用和记录：
- `main/eval.py` - 主评估脚本
- `main_new/eval.py` - 新版评估脚本
- `main_cluda/eval.py` - CLUDA算法评估脚本

评估结果会被记录到日志文件中，格式如下：
```
AUPRC score is : 0.xxxx
Best F1 score is : 0.xxxx
Best Prec score is : 0.xxxx
Best Rec score is : 0.xxxx
```

---

## 混淆矩阵术语说明

在理解评估指标时，需要了解混淆矩阵的基本术语：

| 术语 | 英文 | 含义 |
|-----|------|------|
| TP | True Positive | 真正例：实际为正类，预测为正类 |
| TN | True Negative | 真负例：实际为负类，预测为负类 |
| FP | False Positive | 假正例：实际为负类，预测为正类（误报） |
| FN | False Negative | 假负例：实际为正类，预测为负类（漏报） |

---

## 指标选择建议

### 异常检测任务（类别不平衡）
推荐使用：
1. **AUPRC** - 主要指标，适合类别不平衡
2. **Best F1** - 平衡精确率和召回率
3. **Best Precision & Recall** - 了解模型在最佳阈值下的详细表现

### 多分类任务（类别平衡）
推荐使用：
1. **Macro F1** - 主要指标，平等对待所有类别
2. **Accuracy** - 辅助指标，直观易懂
3. **Weighted F1** - 类别不平衡时使用

### 多分类任务（类别不平衡）
推荐使用：
1. **Weighted F1** - 主要指标，考虑类别样本数量
2. **Macro F1** - 辅助指标，了解各类别平均性能
3. **Accuracy** - 参考指标

---

## 参考

### 相关文件
- `utils/util_progress_log.py` - 评估指标计算实现
- `main/algorithms.py` - 指标选择逻辑
- `main/eval.py` - 评估脚本
- `main_new/eval.py` - 新版评估脚本

### 使用的库
- `sklearn.metrics` - 提供标准评估指标实现
  - `roc_auc_score` - ROC AUC计算
  - `average_precision_score` - AUPRC计算
  - `precision_recall_curve` - PR曲线计算
  - `f1_score` - F1分数计算
  - `accuracy_score` - 准确率计算

### 相关文档
- [Scikit-learn Metrics Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [Precision-Recall vs ROC Curves](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)

---

**文档版本**: 1.0  
**最后更新**: 2024  
**维护者**: DACAD项目组

